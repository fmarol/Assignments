{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "PySpark",
      "name": "pyspark",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "2.7.14",
      "name": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython2",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "ST446_Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmarol/Assignments/blob/master/ST446_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLs-wtfFLe-Z",
        "colab_type": "text"
      },
      "source": [
        "## Assignment 1\n",
        "\n",
        "Username: fmarol\n",
        "\n",
        "All images can be found in the Image directory also uploaded in the GitHub repository.\n",
        "\n",
        "We start by setting up the bucket and cluster. For part 1 & 2 we need to use the author-large.txt file, so we download this from the url link and place it in our bucket. \n",
        "\n",
        "The steps taken to do this are as follows:\n",
        "\n",
        "Set up bucket:\n",
        "\n",
        "```gsutil mb gs://fmarol-bucket```\n",
        "\n",
        "Create cluster:\n",
        "\n",
        "```gcloud beta dataproc clusters create fmarol-cluster --optional-components=ANACONDA,JUPYTER     --image-version=1.3     --enable-component-gateway  --bucket fmarol-bucket --project st446-267513```\n",
        "\n",
        "Into SSH:\n",
        "\n",
        "```gcloud beta compute --project \"st446-267513\" ssh --zone \"europe-west2-a\" \"fmarol-cluster-m\"```\n",
        "\n",
        "Download text file from the URL, and then copy into the bucket:\n",
        "\n",
        "```wget http://webdam.inria.fr/Jorge/files/author-large.txt```\n",
        "\n",
        "```gsutil cp author-large.txt gs://fmarol-bucket```\n",
        "gsutil cp author-large.txt gs://fmarol-bucket\n",
        "The code ran to do this can be seen in Image 1 and 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNT51NNrLe-e",
        "colab_type": "text"
      },
      "source": [
        "### Part 1\n",
        "First we download the data from the text file and transform it so that it has the form of a dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi1LJI1XLe-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_from_file = sc.\\\n",
        "    textFile(\"gs://fmarol-bucket/author-large.txt\", 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "WMY8fASgLe-4",
        "colab_type": "code",
        "colab": {},
        "outputId": "85538de2-f8c9-43fa-81b7-18a42b8be684"
      },
      "source": [
        "data_from_file.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[u'Jurgen Annevelink\\tModern Database Systems\\tObject SQL - A Language for the Design and Implementation of Object Databases.\\t1995',\n",
              " u'Rafiul Ahad\\tModern Database Systems\\tObject SQL - A Language for the Design and Implementation of Object Databases.\\t1995',\n",
              " u'Amelia Carlson\\tModern Database Systems\\tObject SQL - A Language for the Design and Implementation of Object Databases.\\t1995',\n",
              " u'Daniel H. Fishman\\tModern Database Systems\\tObject SQL - A Language for the Design and Implementation of Object Databases.\\t1995',\n",
              " u'Michael L. Heytens\\tModern Database Systems\\tObject SQL - A Language for the Design and Implementation of Object Databases.\\t1995']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhk9ID0JLe_n",
        "colab_type": "code",
        "colab": {},
        "outputId": "3b525ee7-e5d4-430e-e0b5-6ceee1d43104"
      },
      "source": [
        "import numpy as np\n",
        "data_from_file_conv = data_from_file.map(lambda row: np.array(row.strip().split(\"\\t\")))\n",
        "data_from_file_conv.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([u'Jurgen Annevelink', u'Modern Database Systems',\n",
              "        u'Object SQL - A Language for the Design and Implementation of Object Databases.',\n",
              "        u'1995'], dtype='<U78'),\n",
              " array([u'Rafiul Ahad', u'Modern Database Systems',\n",
              "        u'Object SQL - A Language for the Design and Implementation of Object Databases.',\n",
              "        u'1995'], dtype='<U78'),\n",
              " array([u'Amelia Carlson', u'Modern Database Systems',\n",
              "        u'Object SQL - A Language for the Design and Implementation of Object Databases.',\n",
              "        u'1995'], dtype='<U78'),\n",
              " array([u'Daniel H. Fishman', u'Modern Database Systems',\n",
              "        u'Object SQL - A Language for the Design and Implementation of Object Databases.',\n",
              "        u'1995'], dtype='<U78'),\n",
              " array([u'Michael L. Heytens', u'Modern Database Systems',\n",
              "        u'Object SQL - A Language for the Design and Implementation of Object Databases.',\n",
              "        u'1995'], dtype='<U78')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQcYSEvmgQCl",
        "colab_type": "text"
      },
      "source": [
        "Next, we take just the Name and Title columns, saved as ```book_authors```.\n",
        "\n",
        "Then, we join the dataset on itself by the column Title and this gives us all the combinations of pairs of authors, including repeats:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DczWQNbyLfAL",
        "colab_type": "code",
        "colab": {},
        "outputId": "e78d3ee3-6e74-4d51-d299-d67f53e4f87d"
      },
      "source": [
        "book_authors = data_from_file_conv.map(lambda row: (row[2], row[0]))\n",
        "\n",
        "book_authors_join = book_authors.join(book_authors).filter(lambda row: row[1][0]<row[1][1])\n",
        "book_authors_join.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(u'Impact of the Processing Methods on the Performance of the X-ray Film Screen Combinations.',\n",
              "  (u'Cao Hou-de', u'Wang Yong-ming')),\n",
              " (u'Fusing multiple systems into a compact lattice index for chinese spoken term detection.',\n",
              "  (u'Peng Yu', u'Sha Meng')),\n",
              " (u'Fusing multiple systems into a compact lattice index for chinese spoken term detection.',\n",
              "  (u'Jia Liu', u'Sha Meng')),\n",
              " (u'Fusing multiple systems into a compact lattice index for chinese spoken term detection.',\n",
              "  (u'Jia Liu', u'Peng Yu')),\n",
              " (u'Fusing multiple systems into a compact lattice index for chinese spoken term detection.',\n",
              "  (u'Frank Seide', u'Sha Meng'))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V53InYLHgsl5",
        "colab_type": "text"
      },
      "source": [
        "Before counting the number of titles per pair, we check to see if the rows are unique. If they are not then we might count pairs multiple times for one title:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmnOMYTQLfAf",
        "colab_type": "code",
        "colab": {},
        "outputId": "ccfe60ba-7401-4415-8070-303e4c7dea94"
      },
      "source": [
        "book_authors_join.map(lambda row: ((row[0], row[1]), 1)).reduceByKey(lambda x, y: x+y).filter(lambda row: row[1] != 1).take(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((u'Introduction.', (u'James Ohene-Djan', u'Robert Logcher')), 2),\n",
              " ((u'Adaptive Video on Demand.', (u'Amir Herzberg', u'Sudhanshu Aggarwal')),\n",
              "  4),\n",
              " ((u'Minitrack Introduction.', (u'Jrg M. Haake', u'Judith Gebauer')), 2),\n",
              " ((u'Minitrack Introduction.', (u'Robert O. Briggs', u'Samuli Pekkola')), 2),\n",
              " ((u'Minitrack Introduction.',\n",
              "   (u'Christopher P. Holland', u'Mark N. Frolick')),\n",
              "  2),\n",
              " ((u'A Semantic Data Grid for Satellite Mission Quality Analysis.',\n",
              "   (u'Manuel Snchez-Gestido', u'scar Corcho')),\n",
              "  4),\n",
              " ((u'3.', (u'Gershon Elber', u'Yuefei Zhu')), 2),\n",
              " ((u'Minitrack Introduction.', (u'Pirkko Walden', u'Scott McCoy')), 4),\n",
              " ((u'BLOG: Probabilistic Models with Unknown Objects.',\n",
              "   (u'Andrey Kolobov', u'Brian Milch')),\n",
              "  4),\n",
              " ((u'Minitrack Introduction.', (u'Kenneth J. Trimmer', u'Marios Koufaris')),\n",
              "  2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWiBaHPYhDuq",
        "colab_type": "text"
      },
      "source": [
        "As one can see from the output above, the rows are not unique and some pair-title combinations appear more than once.\n",
        "\n",
        "To avoid counting them multiple times, we group by the pair-title combinations, before mapping the pairs to values of one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B38umPsoLfAw",
        "colab_type": "code",
        "colab": {},
        "outputId": "b2045422-1a10-4482-a094-5f0769001631"
      },
      "source": [
        "count = book_authors_join.map(lambda row: ((row[0], row[1]), 1)).groupByKey().mapValues(list).map(lambda row: (row[0][1], 1))\n",
        "count.take(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((u'Don DeSota', u'Sameh Sharkawi'), 1),\n",
              " ((u'Xiao-Yuan Jing', u'Yong-Chuan Zhang'), 1),\n",
              " ((u'Guy-Ren Perrin', u'Wolfgang Karl'), 1),\n",
              " ((u'J. W. Park', u'Y. S. Moon'), 1),\n",
              " ((u'Shuichi Sakai', u'Toshitsugu Yuba'), 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sb-cla4he5H",
        "colab_type": "text"
      },
      "source": [
        "Finally we reduce the author pairs, summing up the mapped values and then sorting the sums in descending order to give the top 10 author pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdVau2iqLfBB",
        "colab_type": "code",
        "colab": {},
        "outputId": "14d26310-7ca1-47a8-b621-baa392f622c5"
      },
      "source": [
        "count.reduceByKey(lambda x, y: x+y).sortBy(lambda row: row[1], ascending = False).take(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((u'Irith Pomeranz', u'Sudhakar M. Reddy'), 246),\n",
              " ((u'Amr El Abbadi', u'Divyakant Agrawal'), 161),\n",
              " ((u'Makoto Takizawa', u'Tomoya Enokido'), 137),\n",
              " ((u'Didier Dubois', u'Henri Prade'), 122),\n",
              " ((u'Elizabeth Chang', u'Tharam S. Dillon'), 115),\n",
              " ((u'Mary Jane Irwin', u'Narayanan Vijaykrishnan'), 107),\n",
              " ((u'Mahmut T. Kandemir', u'Mary Jane Irwin'), 100),\n",
              " ((u'Chun Chen', u'Jiajun Bu'), 99),\n",
              " ((u'Shojiro Nishio', u'Takahiro Hara'), 96),\n",
              " ((u'Filip De Turck', u'Piet Demeester'), 90)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VB9SBECLfBW",
        "colab_type": "text"
      },
      "source": [
        "### Part 2\n",
        "Now we do the same analysis using PySpark:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHQ4QagiLfBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'gs://fmarol-bucket/author-large.txt'\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"author\", StringType(), True),    \n",
        "    StructField(\"journal\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"year\", LongType(), True)\n",
        "])\n",
        "\n",
        "author_large = spark.read.csv(filename, \n",
        "                    header='false', schema=schema, sep='\\t')\n",
        "author_large.createOrReplaceTempView(\"author_large\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1gbocDwh-iU",
        "colab_type": "text"
      },
      "source": [
        "The Spark SQL code does something very similar as the operations with the RDD.\n",
        "\n",
        "First a temporary table ```all_combos``` is created by joining the dataset on itself by the title column. From this, the author pair and total number of distinct titles per author pair are extracted and then ordered descending by total number.\n",
        "\n",
        "The same results are seen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVONL6DyLfBq",
        "colab_type": "code",
        "colab": {},
        "outputId": "e5eedd15-af1b-4580-9d75-e0aa2959683b"
      },
      "source": [
        "spark.sql(\"\"\"with all_combos as (select CONCAT(a2.author, ', ', a1.author) author_pair, a1.title\n",
        "        from author_large a1 join author_large a2 on a1.title=a2.title where a1.author>a2.author)\n",
        "        select author_pair, count(distinct title) count from all_combos group by author_pair order by count desc\"\"\").show(10, truncate = False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------+-----+\n",
            "|author_pair                             |count|\n",
            "+----------------------------------------+-----+\n",
            "|Irith Pomeranz, Sudhakar M. Reddy       |246  |\n",
            "|Amr El Abbadi, Divyakant Agrawal        |161  |\n",
            "|Makoto Takizawa, Tomoya Enokido         |137  |\n",
            "|Didier Dubois, Henri Prade              |122  |\n",
            "|Elizabeth Chang, Tharam S. Dillon       |115  |\n",
            "|Mary Jane Irwin, Narayanan Vijaykrishnan|107  |\n",
            "|Mahmut T. Kandemir, Mary Jane Irwin     |100  |\n",
            "|Chun Chen, Jiajun Bu                    |99   |\n",
            "|Shojiro Nishio, Takahiro Hara           |96   |\n",
            "|Filip De Turck, Piet Demeester          |90   |\n",
            "+----------------------------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAtu7sHtLfB7",
        "colab_type": "text"
      },
      "source": [
        "### Part 3\n",
        "\n",
        "For this part we use a different dataset, Yelp, which is contained in a zipfile on DropBox. To in order to load the JSON file into a database in Hive, a SerDe must be used. The SerDe is contained in a JAR file which can be downloaded from this website: https://mvnrepository.com/artifact/org.apache.hive.hcatalog/hive-hcatalog-core/0.13.0.\n",
        "\n",
        "First both the Yelp JSON file and JAR file are downloaded to the local drive, and then uploaded to the bucket on GCP:\n",
        "\n",
        "```gsutil cp 'D:/yelp_academic_dataset_user.json' gs://fmarol-bucket```\n",
        "\n",
        "```gsutil cp 'C:/Users/loram/OneDrive/Desktop/LSE MSc/ST446/hive-hcatalog-core-3.1.2.jar' gs://fmarol-bucket```\n",
        "\n",
        "Into SSH\n",
        "\n",
        "```gcloud beta compute --project \"st446-513267\" ssh --zone \"europe-west2-a\" \"fmarol-cluster-m\"```\n",
        "\n",
        "The Yelp file and JAR file are then copied onto the .....:\n",
        "\n",
        "```gsutil cp gs://fmarol-bucket/yelp_academic_dataset_user.json .```\n",
        "\n",
        "```gsutil cp gs://fmarol-bucket/hive-hcatalog-core-3.1.2.jar .```\n",
        "\n",
        "Then the files are put on the HDFS server:\n",
        "\n",
        "hadoop -put yelp_academic_dataset_user.json /\n",
        "\n",
        "hadoop -put hive-hcatalog-core-3.1.2.jar /\n",
        "\n",
        "Then we enter Hive:\n",
        "\n",
        "```hive```\n",
        "\n",
        "The following code is then ran in order to:\n",
        "- add the jar file that contains the SerDe for the JSON file\n",
        "\n",
        "```ADD JAR hive-hcatalog-core-3.1.2.jar;```\n",
        "\n",
        "- create a database to hold the Yelp dataset in\n",
        "\n",
        "```CREATE DATABASE IF NOT EXISTS yelp_user_database;```\n",
        "\n",
        "- create an empty table, with the schema of the Yelp JSON file, and the appropriate SerDe file path\n",
        "\n",
        "```USE yelp_user_database;```\n",
        "\n",
        "```\n",
        "CREATE TABLE IF NOT EXISTS yelp_users (\n",
        "  user_id STRING,\n",
        "  name STRING,\n",
        "  review_count INT,\n",
        "  yelping_since STRING,\n",
        "  friends ARRAY<STRING>,\n",
        "  useful INT,\n",
        "  funny INT,\n",
        "  cool INT,\n",
        "  fans INT,\n",
        "  elite ARRAY<STRING>,\n",
        "  verage_stars FLOAT,\n",
        "  compliment_hot INT,\n",
        "  compliment_more INT,\n",
        "  compliment_profile INT,\n",
        "  compliment_cute INT,\n",
        "  compliment_list INT,\n",
        "  compliment_note INT,\n",
        "  compliment_plain INT,\n",
        "  compliment_cool INT,\n",
        "  compliment_funny INT,\n",
        "  compliment_writer INT,\n",
        "  compliment_photos INT,\n",
        "  type STRING ) \n",
        "ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe';\n",
        "```\n",
        "\n",
        "- load the JSON file into the empty table\n",
        "\n",
        "```LOAD DATA LOCAL INPATH 'yelp_academic_dataset_user.json' INTO TABLE yelp_users;```\n",
        "\n",
        "The above code has been run and outputs can be seen in Images 3 and 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj3IqTmfbDp_",
        "colab_type": "text"
      },
      "source": [
        "Now that the dataset has been prepared, we answer the questions. We enter the database:\n",
        "```USE yelp_database;```\n",
        "\n",
        "1. Retrieve the schema for the dataset:\n",
        "\n",
        "```DESCRIBE yelp_users;```\n",
        "\n",
        "2. Count the number of rows in the dataset:\n",
        "\n",
        "```SELECT COUNT(*) FROM yelp_users;```\n",
        "\n",
        "3. Select top 10 users who have provided the largest number of reviews\n",
        "\n",
        "First we check to see that there are no repeated user_ids by counting the distinct user_ids and seeing if this is equal to the number of rows.\n",
        "\n",
        "```SELECT COUNT(DISTINCT user_id) FROM yelp_users;```\n",
        "\n",
        "Seeing that these are equal, we go on to rank the users by number of reviews:\n",
        "\n",
        "```SELECT name, review_count FROM yelp_users ORDER BY review_count DESC LIMIT 10;```\n",
        "\n",
        "All above code and outputs can be seen in Images 5 to 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiHnO0qdLfCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}